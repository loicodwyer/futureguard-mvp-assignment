{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 - 31s - loss: 0.2357 - accuracy: 0.9209 - val_loss: 0.1673 - val_accuracy: 0.9565 - 31s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "20/20 - 24s - loss: 0.1294 - accuracy: 0.9632 - val_loss: 0.0910 - val_accuracy: 0.9707 - 24s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "20/20 - 23s - loss: 0.0441 - accuracy: 0.9893 - val_loss: 0.0434 - val_accuracy: 0.9891 - 23s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "20/20 - 24s - loss: 0.0222 - accuracy: 0.9950 - val_loss: 0.0305 - val_accuracy: 0.9854 - 24s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "20/20 - 23s - loss: 0.0181 - accuracy: 0.9953 - val_loss: 0.0396 - val_accuracy: 0.9812 - 23s/epoch - 1s/step\n",
      "Epoch 6/50\n",
      "20/20 - 23s - loss: 0.0154 - accuracy: 0.9958 - val_loss: 0.0416 - val_accuracy: 0.9842 - 23s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "20/20 - 25s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0486 - val_accuracy: 0.9871 - 25s/epoch - 1s/step\n",
      "Epoch 8/50\n",
      "20/20 - 23s - loss: 0.0122 - accuracy: 0.9964 - val_loss: 0.0393 - val_accuracy: 0.9854 - 23s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "20/20 - 24s - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0315 - val_accuracy: 0.9853 - 24s/epoch - 1s/step\n",
      "Epoch 10/50\n",
      "20/20 - 24s - loss: 0.0100 - accuracy: 0.9965 - val_loss: 0.0306 - val_accuracy: 0.9902 - 24s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "20/20 - 25s - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.0125 - val_accuracy: 0.9954 - 25s/epoch - 1s/step\n",
      "Epoch 12/50\n",
      "20/20 - 26s - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.0139 - val_accuracy: 0.9943 - 26s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "20/20 - 25s - loss: 0.0083 - accuracy: 0.9968 - val_loss: 0.0101 - val_accuracy: 0.9963 - 25s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "20/20 - 24s - loss: 0.0083 - accuracy: 0.9966 - val_loss: 0.0134 - val_accuracy: 0.9948 - 24s/epoch - 1s/step\n",
      "Epoch 15/50\n",
      "20/20 - 25s - loss: 0.0081 - accuracy: 0.9967 - val_loss: 0.0066 - val_accuracy: 0.9972 - 25s/epoch - 1s/step\n",
      "Epoch 16/50\n",
      "20/20 - 27s - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.0199 - val_accuracy: 0.9926 - 27s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "20/20 - 27s - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.0094 - val_accuracy: 0.9952 - 27s/epoch - 1s/step\n",
      "Epoch 18/50\n",
      "20/20 - 25s - loss: 0.0076 - accuracy: 0.9970 - val_loss: 0.0480 - val_accuracy: 0.9886 - 25s/epoch - 1s/step\n",
      "Epoch 19/50\n",
      "20/20 - 28s - loss: 0.0070 - accuracy: 0.9968 - val_loss: 0.0141 - val_accuracy: 0.9943 - 28s/epoch - 1s/step\n",
      "Epoch 20/50\n",
      "20/20 - 26s - loss: 0.0066 - accuracy: 0.9973 - val_loss: 0.0080 - val_accuracy: 0.9967 - 26s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "20/20 - 25s - loss: 0.0061 - accuracy: 0.9972 - val_loss: 0.0087 - val_accuracy: 0.9971 - 25s/epoch - 1s/step\n",
      "Epoch 22/50\n",
      "20/20 - 26s - loss: 0.0054 - accuracy: 0.9976 - val_loss: 0.0069 - val_accuracy: 0.9980 - 26s/epoch - 1s/step\n",
      "Epoch 23/50\n",
      "20/20 - 25s - loss: 0.0052 - accuracy: 0.9978 - val_loss: 0.0042 - val_accuracy: 0.9983 - 25s/epoch - 1s/step\n",
      "Epoch 24/50\n",
      "20/20 - 25s - loss: 0.0052 - accuracy: 0.9977 - val_loss: 0.0066 - val_accuracy: 0.9963 - 25s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "20/20 - 26s - loss: 0.0051 - accuracy: 0.9979 - val_loss: 0.0195 - val_accuracy: 0.9934 - 26s/epoch - 1s/step\n",
      "Epoch 26/50\n",
      "20/20 - 25s - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0047 - val_accuracy: 0.9980 - 25s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "20/20 - 25s - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0097 - val_accuracy: 0.9963 - 25s/epoch - 1s/step\n",
      "Epoch 28/50\n",
      "20/20 - 26s - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0377 - val_accuracy: 0.9913 - 26s/epoch - 1s/step\n",
      "Epoch 29/50\n",
      "20/20 - 25s - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0068 - val_accuracy: 0.9963 - 25s/epoch - 1s/step\n",
      "Epoch 30/50\n",
      "20/20 - 25s - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0035 - val_accuracy: 0.9982 - 25s/epoch - 1s/step\n",
      "Epoch 31/50\n",
      "20/20 - 26s - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0150 - val_accuracy: 0.9950 - 26s/epoch - 1s/step\n",
      "Epoch 32/50\n",
      "20/20 - 26s - loss: 0.0032 - accuracy: 0.9986 - val_loss: 0.0051 - val_accuracy: 0.9976 - 26s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "20/20 - 27s - loss: 0.0025 - accuracy: 0.9990 - val_loss: 0.0328 - val_accuracy: 0.9919 - 27s/epoch - 1s/step\n",
      "Epoch 34/50\n",
      "20/20 - 26s - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0029 - val_accuracy: 0.9993 - 26s/epoch - 1s/step\n",
      "Epoch 35/50\n",
      "20/20 - 25s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0035 - val_accuracy: 0.9985 - 25s/epoch - 1s/step\n",
      "Epoch 36/50\n",
      "20/20 - 25s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0088 - val_accuracy: 0.9972 - 25s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "20/20 - 26s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0110 - val_accuracy: 0.9969 - 26s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "20/20 - 25s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0071 - val_accuracy: 0.9978 - 25s/epoch - 1s/step\n",
      "Epoch 39/50\n",
      "20/20 - 26s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0048 - val_accuracy: 0.9985 - 26s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "20/20 - 25s - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0076 - val_accuracy: 0.9969 - 25s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "20/20 - 25s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0040 - val_accuracy: 0.9991 - 25s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "20/20 - 24s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 5.8297e-04 - val_accuracy: 0.9996 - 24s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "20/20 - 25s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0091 - val_accuracy: 0.9967 - 25s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "20/20 - 25s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0239 - val_accuracy: 0.9924 - 25s/epoch - 1s/step\n",
      "Epoch 45/50\n",
      "20/20 - 26s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 0.9991 - 26s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "20/20 - 26s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 3.8058e-05 - val_accuracy: 1.0000 - 26s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "20/20 - 25s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0096 - val_accuracy: 0.9959 - 25s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "20/20 - 24s - loss: 0.0014 - accuracy: 0.9994 - val_loss: 3.4785e-05 - val_accuracy: 1.0000 - 24s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "20/20 - 26s - loss: 9.7235e-04 - accuracy: 0.9997 - val_loss: 0.0198 - val_accuracy: 0.9934 - 26s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "20/20 - 26s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0316 - val_accuracy: 0.9919 - 26s/epoch - 1s/step\n",
      "Epoch 1/50\n",
      "20/20 - 58s - loss: 0.0081 - val_loss: 0.0049 - 58s/epoch - 3s/step\n",
      "Epoch 2/50\n",
      "20/20 - 46s - loss: 0.0033 - val_loss: 0.0031 - 46s/epoch - 2s/step\n",
      "Epoch 3/50\n",
      "20/20 - 45s - loss: 0.0027 - val_loss: 0.0027 - 45s/epoch - 2s/step\n",
      "Epoch 4/50\n",
      "20/20 - 45s - loss: 0.0024 - val_loss: 0.0029 - 45s/epoch - 2s/step\n",
      "Epoch 5/50\n",
      "20/20 - 45s - loss: 0.0020 - val_loss: 0.0021 - 45s/epoch - 2s/step\n",
      "Epoch 6/50\n",
      "20/20 - 44s - loss: 0.0016 - val_loss: 0.0019 - 44s/epoch - 2s/step\n",
      "Epoch 7/50\n",
      "20/20 - 46s - loss: 0.0014 - val_loss: 0.0020 - 46s/epoch - 2s/step\n",
      "Epoch 8/50\n",
      "20/20 - 47s - loss: 0.0013 - val_loss: 0.0015 - 47s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "20/20 - 44s - loss: 0.0012 - val_loss: 0.0011 - 44s/epoch - 2s/step\n",
      "Epoch 10/50\n",
      "20/20 - 47s - loss: 0.0012 - val_loss: 0.0013 - 47s/epoch - 2s/step\n",
      "Epoch 11/50\n",
      "20/20 - 46s - loss: 9.4568e-04 - val_loss: 0.0012 - 46s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "20/20 - 45s - loss: 9.7581e-04 - val_loss: 0.0016 - 45s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "20/20 - 45s - loss: 0.0010 - val_loss: 0.0015 - 45s/epoch - 2s/step\n",
      "Epoch 14/50\n",
      "20/20 - 48s - loss: 9.3632e-04 - val_loss: 8.4476e-04 - 48s/epoch - 2s/step\n",
      "Epoch 15/50\n",
      "20/20 - 44s - loss: 8.7498e-04 - val_loss: 0.0011 - 44s/epoch - 2s/step\n",
      "Epoch 16/50\n",
      "20/20 - 43s - loss: 7.8845e-04 - val_loss: 0.0013 - 43s/epoch - 2s/step\n",
      "Epoch 17/50\n",
      "20/20 - 44s - loss: 7.4699e-04 - val_loss: 6.8167e-04 - 44s/epoch - 2s/step\n",
      "Epoch 18/50\n",
      "20/20 - 46s - loss: 6.6818e-04 - val_loss: 6.0729e-04 - 46s/epoch - 2s/step\n",
      "Epoch 19/50\n",
      "20/20 - 43s - loss: 7.3433e-04 - val_loss: 7.1427e-04 - 43s/epoch - 2s/step\n",
      "Epoch 20/50\n",
      "20/20 - 43s - loss: 6.7106e-04 - val_loss: 8.6532e-04 - 43s/epoch - 2s/step\n",
      "Epoch 21/50\n",
      "20/20 - 43s - loss: 7.3574e-04 - val_loss: 9.4416e-04 - 43s/epoch - 2s/step\n",
      "Epoch 22/50\n",
      "20/20 - 43s - loss: 8.7271e-04 - val_loss: 0.0013 - 43s/epoch - 2s/step\n",
      "Epoch 23/50\n",
      "20/20 - 44s - loss: 7.6987e-04 - val_loss: 5.0498e-04 - 44s/epoch - 2s/step\n",
      "Epoch 24/50\n",
      "20/20 - 46s - loss: 7.1207e-04 - val_loss: 9.1420e-04 - 46s/epoch - 2s/step\n",
      "Epoch 25/50\n",
      "20/20 - 45s - loss: 6.2892e-04 - val_loss: 8.8727e-04 - 45s/epoch - 2s/step\n",
      "Epoch 26/50\n",
      "20/20 - 45s - loss: 5.9813e-04 - val_loss: 7.1781e-04 - 45s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "20/20 - 46s - loss: 5.2877e-04 - val_loss: 5.3615e-04 - 46s/epoch - 2s/step\n",
      "Epoch 28/50\n",
      "20/20 - 46s - loss: 4.8641e-04 - val_loss: 4.7680e-04 - 46s/epoch - 2s/step\n",
      "Epoch 29/50\n",
      "20/20 - 44s - loss: 5.2759e-04 - val_loss: 6.6885e-04 - 44s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "20/20 - 43s - loss: 5.4897e-04 - val_loss: 5.3618e-04 - 43s/epoch - 2s/step\n",
      "Epoch 31/50\n",
      "20/20 - 43s - loss: 5.3863e-04 - val_loss: 5.2236e-04 - 43s/epoch - 2s/step\n",
      "Epoch 32/50\n",
      "20/20 - 43s - loss: 4.9342e-04 - val_loss: 6.6492e-04 - 43s/epoch - 2s/step\n",
      "Epoch 33/50\n",
      "20/20 - 44s - loss: 5.7927e-04 - val_loss: 5.1703e-04 - 44s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "20/20 - 44s - loss: 4.4902e-04 - val_loss: 6.0828e-04 - 44s/epoch - 2s/step\n",
      "Epoch 35/50\n",
      "20/20 - 44s - loss: 4.5841e-04 - val_loss: 6.8460e-04 - 44s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "20/20 - 42s - loss: 5.6190e-04 - val_loss: 8.3189e-04 - 42s/epoch - 2s/step\n",
      "Epoch 37/50\n",
      "20/20 - 47s - loss: 5.2548e-04 - val_loss: 6.1201e-04 - 47s/epoch - 2s/step\n",
      "Epoch 38/50\n",
      "20/20 - 40s - loss: 3.8494e-04 - val_loss: 5.0689e-04 - 40s/epoch - 2s/step\n",
      "Epoch 1/50\n",
      "20/20 - 51s - loss: 0.1953 - val_loss: 0.0775 - 51s/epoch - 3s/step\n",
      "Epoch 2/50\n",
      "20/20 - 38s - loss: 0.0740 - val_loss: 0.0426 - 38s/epoch - 2s/step\n",
      "Epoch 3/50\n",
      "20/20 - 40s - loss: 0.0574 - val_loss: 0.0352 - 40s/epoch - 2s/step\n",
      "Epoch 4/50\n",
      "20/20 - 40s - loss: 0.0456 - val_loss: 0.0276 - 40s/epoch - 2s/step\n",
      "Epoch 5/50\n",
      "20/20 - 41s - loss: 0.0376 - val_loss: 0.0257 - 41s/epoch - 2s/step\n",
      "Epoch 6/50\n",
      "20/20 - 39s - loss: 0.0346 - val_loss: 0.0335 - 39s/epoch - 2s/step\n",
      "Epoch 7/50\n",
      "20/20 - 39s - loss: 0.0417 - val_loss: 0.0310 - 39s/epoch - 2s/step\n",
      "Epoch 8/50\n",
      "20/20 - 40s - loss: 0.0332 - val_loss: 0.0190 - 40s/epoch - 2s/step\n",
      "Epoch 9/50\n",
      "20/20 - 48s - loss: 0.0246 - val_loss: 0.0162 - 48s/epoch - 2s/step\n",
      "Epoch 10/50\n",
      "20/20 - 44s - loss: 0.0223 - val_loss: 0.0218 - 44s/epoch - 2s/step\n",
      "Epoch 11/50\n",
      "20/20 - 42s - loss: 0.0255 - val_loss: 0.0166 - 42s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "20/20 - 40s - loss: 0.0274 - val_loss: 0.0168 - 40s/epoch - 2s/step\n",
      "Epoch 13/50\n",
      "20/20 - 42s - loss: 0.0194 - val_loss: 0.0170 - 42s/epoch - 2s/step\n",
      "Epoch 14/50\n",
      "20/20 - 43s - loss: 0.0171 - val_loss: 0.0130 - 43s/epoch - 2s/step\n",
      "Epoch 15/50\n",
      "20/20 - 44s - loss: 0.0147 - val_loss: 0.0094 - 44s/epoch - 2s/step\n",
      "Epoch 16/50\n",
      "20/20 - 42s - loss: 0.0138 - val_loss: 0.0092 - 42s/epoch - 2s/step\n",
      "Epoch 17/50\n",
      "20/20 - 41s - loss: 0.0164 - val_loss: 0.0088 - 41s/epoch - 2s/step\n",
      "Epoch 18/50\n",
      "20/20 - 42s - loss: 0.0160 - val_loss: 0.0157 - 42s/epoch - 2s/step\n",
      "Epoch 19/50\n",
      "20/20 - 44s - loss: 0.0150 - val_loss: 0.0098 - 44s/epoch - 2s/step\n",
      "Epoch 20/50\n",
      "20/20 - 41s - loss: 0.0138 - val_loss: 0.0104 - 41s/epoch - 2s/step\n",
      "Epoch 21/50\n",
      "20/20 - 42s - loss: 0.0167 - val_loss: 0.0143 - 42s/epoch - 2s/step\n",
      "Epoch 22/50\n",
      "20/20 - 42s - loss: 0.0162 - val_loss: 0.0078 - 42s/epoch - 2s/step\n",
      "Epoch 23/50\n",
      "20/20 - 42s - loss: 0.0128 - val_loss: 0.0084 - 42s/epoch - 2s/step\n",
      "Epoch 24/50\n",
      "20/20 - 42s - loss: 0.0119 - val_loss: 0.0079 - 42s/epoch - 2s/step\n",
      "Epoch 25/50\n",
      "20/20 - 41s - loss: 0.0115 - val_loss: 0.0074 - 41s/epoch - 2s/step\n",
      "Epoch 26/50\n",
      "20/20 - 41s - loss: 0.0107 - val_loss: 0.0074 - 41s/epoch - 2s/step\n",
      "Epoch 27/50\n",
      "20/20 - 41s - loss: 0.0105 - val_loss: 0.0073 - 41s/epoch - 2s/step\n",
      "Epoch 28/50\n",
      "20/20 - 41s - loss: 0.0111 - val_loss: 0.0093 - 41s/epoch - 2s/step\n",
      "Epoch 29/50\n",
      "20/20 - 41s - loss: 0.0156 - val_loss: 0.0097 - 41s/epoch - 2s/step\n",
      "Epoch 30/50\n",
      "20/20 - 40s - loss: 0.0127 - val_loss: 0.0083 - 40s/epoch - 2s/step\n",
      "Epoch 31/50\n",
      "20/20 - 41s - loss: 0.0113 - val_loss: 0.0080 - 41s/epoch - 2s/step\n",
      "Epoch 32/50\n",
      "20/20 - 41s - loss: 0.0118 - val_loss: 0.0129 - 41s/epoch - 2s/step\n",
      "Epoch 33/50\n",
      "20/20 - 41s - loss: 0.0142 - val_loss: 0.0144 - 41s/epoch - 2s/step\n",
      "Epoch 34/50\n",
      "20/20 - 42s - loss: 0.0121 - val_loss: 0.0079 - 42s/epoch - 2s/step\n",
      "Epoch 35/50\n",
      "20/20 - 42s - loss: 0.0116 - val_loss: 0.0113 - 42s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "20/20 - 42s - loss: 0.0118 - val_loss: 0.0078 - 42s/epoch - 2s/step\n",
      "Epoch 37/50\n",
      "20/20 - 42s - loss: 0.0098 - val_loss: 0.0068 - 42s/epoch - 2s/step\n",
      "Epoch 38/50\n",
      "20/20 - 41s - loss: 0.0094 - val_loss: 0.0068 - 41s/epoch - 2s/step\n",
      "Epoch 39/50\n",
      "20/20 - 42s - loss: 0.0094 - val_loss: 0.0070 - 42s/epoch - 2s/step\n",
      "Epoch 40/50\n",
      "20/20 - 42s - loss: 0.0101 - val_loss: 0.0070 - 42s/epoch - 2s/step\n",
      "Epoch 41/50\n",
      "20/20 - 41s - loss: 0.0099 - val_loss: 0.0073 - 41s/epoch - 2s/step\n",
      "Epoch 42/50\n",
      "20/20 - 42s - loss: 0.0112 - val_loss: 0.0119 - 42s/epoch - 2s/step\n",
      "Epoch 43/50\n",
      "20/20 - 41s - loss: 0.0109 - val_loss: 0.0078 - 41s/epoch - 2s/step\n",
      "Epoch 44/50\n",
      "20/20 - 41s - loss: 0.0112 - val_loss: 0.0076 - 41s/epoch - 2s/step\n",
      "Epoch 45/50\n",
      "20/20 - 41s - loss: 0.0104 - val_loss: 0.0086 - 41s/epoch - 2s/step\n",
      "Epoch 46/50\n",
      "20/20 - 45s - loss: 0.0095 - val_loss: 0.0076 - 45s/epoch - 2s/step\n",
      "Epoch 47/50\n",
      "20/20 - 43s - loss: 0.0096 - val_loss: 0.0066 - 43s/epoch - 2s/step\n",
      "Epoch 48/50\n",
      "20/20 - 43s - loss: 0.0099 - val_loss: 0.0094 - 43s/epoch - 2s/step\n",
      "Epoch 49/50\n",
      "20/20 - 42s - loss: 0.0108 - val_loss: 0.0092 - 42s/epoch - 2s/step\n",
      "Epoch 50/50\n",
      "20/20 - 40s - loss: 0.0103 - val_loss: 0.0091 - 40s/epoch - 2s/step\n",
      "1/1 [==============================] - 1s 897ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021CA8198CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 761ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021CA8199C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "         date  income_prob   income_p10  income_median  income_p90  \\\n",
      "0  2025-03-01     0.999559  3028.631104    3240.984131  3527.89209   \n",
      "1  2025-03-02     0.000010     0.000000       0.000000     0.00000   \n",
      "2  2025-03-03     0.000010     0.000000       0.000000     0.00000   \n",
      "3  2025-03-04     0.000010     0.000000       0.000000     0.00000   \n",
      "4  2025-03-05     0.000009     0.000000       0.000000     0.00000   \n",
      "\n",
      "   expense_p10  expense_median  expense_p90  \n",
      "0 -1973.733521    -2222.467041 -2478.180420  \n",
      "1     0.000000      -16.045710  -167.809753  \n",
      "2     0.000000      -15.400554  -172.632507  \n",
      "3     0.000000      -19.677402  -172.164963  \n",
      "4     0.000000      -16.173855  -176.217239  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from dateutil.easter import easter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention\n",
    "from tensorflow.keras.layers import LayerNormalization, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Reproducibility\n",
    "# ---------------------------------------------------------------------\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Load data for user_id = 10\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_csv(\n",
    "    r\"C:/Users/loics/OneDrive/Documents/1. BAM/BLOCK 5/Assignment coding/synthetic_transactions_daily_enriched.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "user_df = df[df[\"user_id\"] == 10].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Calendar + holiday features\n",
    "# ---------------------------------------------------------------------\n",
    "user_df[\"day_of_week\"]       = user_df[\"date\"].dt.weekday   # 0 = Monday\n",
    "user_df[\"month\"]             = user_df[\"date\"].dt.month\n",
    "user_df[\"day_of_month\"]      = user_df[\"date\"].dt.day\n",
    "user_df[\"is_first_of_month\"] = (user_df[\"day_of_month\"] == 1).astype(int)\n",
    "\n",
    "def get_nl_holidays(start_year, end_year):\n",
    "    hol = set()\n",
    "    for yr in range(start_year, end_year + 1):\n",
    "        hol.add(date(yr, 1, 1))          # New Year\n",
    "        e = easter(yr)\n",
    "        hol |= {e - timedelta(2), e + timedelta(1)}           # Good Fri / Easter Mon\n",
    "        hol |= {e + timedelta(39), e + timedelta(50)}         # Ascension / Whit Mon\n",
    "        kday = date(yr, 4, 26 if date(yr, 4, 27).weekday() == 6 else 27)\n",
    "        hol.add(kday)                                         # King's Day\n",
    "        if yr == 2025: hol.add(date(yr, 5, 5))                # Liberation\n",
    "        hol |= {date(yr, 12, 25), date(yr, 12, 26)}           # Christmas\n",
    "    return hol\n",
    "\n",
    "holidays = get_nl_holidays(2022, 2025)\n",
    "user_df[\"holiday\"] = user_df[\"date\"].dt.date.isin(holidays).astype(int)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Forecast horizon & window sizes\n",
    "# ---------------------------------------------------------------------\n",
    "forecast_start = pd.Timestamp(\"2025-03-01\")\n",
    "forecast_end   = pd.Timestamp(\"2025-05-31\")\n",
    "H = (forecast_end - forecast_start).days + 1   # 92 days\n",
    "L = 365                                        # 1-year history window\n",
    "\n",
    "train_df = user_df[user_df[\"date\"] < forecast_start].copy()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Prepare numeric series\n",
    "# ---------------------------------------------------------------------\n",
    "income_series  = train_df[\"income\"].values                     # ≥ 0\n",
    "expense_series = train_df[\"expenses\"].values                   # ≤ 0\n",
    "\n",
    "num_train = len(train_df)\n",
    "\n",
    "# one-hot weekday & month\n",
    "dow = np.zeros((num_train, 7),  dtype=np.float32)\n",
    "mon = np.zeros((num_train, 12), dtype=np.float32)\n",
    "dow[np.arange(num_train), train_df[\"day_of_week\"]] = 1.0\n",
    "mon[np.arange(num_train), train_df[\"month\"] - 1]   = 1.0\n",
    "\n",
    "first_of_month = train_df[\"is_first_of_month\"].astype(np.float32).values.reshape(-1, 1)\n",
    "holiday_flag   = train_df[\"holiday\"].astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. Train / validation window indices\n",
    "# ---------------------------------------------------------------------\n",
    "train_idx, val_idx = [], []\n",
    "for s in range(num_train - L - H + 1):\n",
    "    end_date = train_df.iloc[s + L + H - 1][\"date\"]\n",
    "    (train_idx if end_date <= pd.Timestamp(\"2024-12-31\") else val_idx).append(s)\n",
    "train_idx, val_idx = np.array(train_idx), np.array(val_idx)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6. Scaling\n",
    "# ---------------------------------------------------------------------\n",
    "income_max       = income_series.max() or 1.0             # avoid div/0\n",
    "expense_min_abs  = abs(expense_series.min()) or 1.0       # most negative magnitude\n",
    "\n",
    "income_scaled  = income_series  / income_max              # [0, 1]\n",
    "expense_scaled = expense_series / (-expense_min_abs)      # [-1, 0]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7. Dataset builder\n",
    "# ---------------------------------------------------------------------\n",
    "def make_xy(idxs):\n",
    "    Xe, Xd = [], []\n",
    "    y_inc_flag, y_inc_amt, y_exp = [], [], []\n",
    "    for i in idxs:\n",
    "        hist = slice(i,      i + L)\n",
    "        fut  = slice(i + L,  i + L + H)\n",
    "\n",
    "        Xe.append(np.hstack([\n",
    "            income_scaled[hist].reshape(L,1),\n",
    "            expense_scaled[hist].reshape(L,1),\n",
    "            dow[hist], mon[hist], first_of_month[hist], holiday_flag[hist]\n",
    "        ]))\n",
    "        Xd.append(np.hstack([\n",
    "            dow[fut], mon[fut], first_of_month[fut], holiday_flag[fut]\n",
    "        ]))\n",
    "\n",
    "        # targets\n",
    "        inc_fut_scaled = income_scaled[fut]\n",
    "        y_inc_flag.append((inc_fut_scaled > 0).astype(np.float32))\n",
    "        y_inc_amt.append(inc_fut_scaled)\n",
    "        y_exp.append(expense_scaled[fut])\n",
    "    return (np.array(Xe, dtype=np.float32),\n",
    "            np.array(Xd, dtype=np.float32),\n",
    "            np.array(y_inc_flag, dtype=np.float32),\n",
    "            np.array(y_inc_amt,  dtype=np.float32),\n",
    "            np.array(y_exp,      dtype=np.float32))\n",
    "\n",
    "Xe_tr, Xd_tr, y_flag_tr, y_inc_tr, y_exp_tr = make_xy(train_idx)\n",
    "Xe_val, Xd_val, y_flag_val, y_inc_val, y_exp_val = (\n",
    "    make_xy(val_idx) if len(val_idx) else (None,)*5\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8. Loss functions\n",
    "# ---------------------------------------------------------------------\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "quantiles = tf.constant([0.1, 0.5, 0.9], dtype=tf.float32)\n",
    "Q = len(quantiles)\n",
    "\n",
    "def masked_pinball_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Pinball loss on positive-income days only (y_true==0 ⇒ no loss).\n",
    "    \"\"\"\n",
    "    # y_true / y_pred shape = (B, H, Q)\n",
    "    mask = tf.cast(tf.greater(y_true[..., 0], 0), tf.float32)  # shape (B, H)\n",
    "    e = y_true - y_pred                                        # (B,H,Q)\n",
    "    q = quantiles[None, None, :]                               # broadcast\n",
    "    loss = tf.maximum(q*e, (q-1)*e)                            # pinball\n",
    "    loss = loss * mask[..., None]                              # zero out on no-income days\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def pinball_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Standard pinball for expenses (always negative, but we don't mask).\n",
    "    \"\"\"\n",
    "    e = y_true - y_pred\n",
    "    q = quantiles[None, None, :]\n",
    "    return tf.reduce_mean(tf.maximum(q*e, (q-1)*e))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9. Transformer factory\n",
    "# ---------------------------------------------------------------------\n",
    "def build_transformer(L, H, enc_dim, dec_dim,\n",
    "                      d_model=64, n_heads=4,\n",
    "                      ff_dim=256, do_rate=0.1, n_layers=2,\n",
    "                      last_activation=\"linear\", output_units=1):\n",
    "    enc_in = Input((L,  enc_dim))\n",
    "    dec_in = Input((H,  dec_dim))\n",
    "\n",
    "    # sinusoidal positional encoding\n",
    "    def pos_enc(T):\n",
    "        pos = np.arange(T)[:, None]\n",
    "        i   = np.arange(d_model)[None, :]\n",
    "        angle = pos / (10000 ** (2*(i//2)/d_model))\n",
    "        pe = np.zeros((T, d_model), dtype=np.float32)\n",
    "        pe[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "        return tf.constant(pe)\n",
    "\n",
    "    x_enc = Dense(d_model)(enc_in) + pos_enc(L)\n",
    "    x_dec = Dense(d_model)(dec_in) + pos_enc(H)\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        # encoder\n",
    "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model,\n",
    "                                  dropout=do_rate)(x_enc, x_enc)\n",
    "        x_enc = LayerNormalization(epsilon=1e-6)(Add()([x_enc, Dropout(do_rate)(attn)]))\n",
    "        ff = Dense(ff_dim, 'relu')(x_enc); ff = Dense(d_model)(ff)\n",
    "        x_enc = LayerNormalization(epsilon=1e-6)(Add()([x_enc, Dropout(do_rate)(ff)]))\n",
    "        # decoder\n",
    "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=d_model,\n",
    "                                  dropout=do_rate)(x_dec, x_dec,\n",
    "                                                   use_causal_mask=True)\n",
    "        x_dec = LayerNormalization(epsilon=1e-6)(Add()([x_dec, Dropout(do_rate)(attn)]))\n",
    "        cross = MultiHeadAttention(num_heads=n_heads, key_dim=d_model,\n",
    "                                   dropout=do_rate)(x_dec, x_enc, x_enc)\n",
    "        x_dec = LayerNormalization(epsilon=1e-6)(Add()([x_dec, Dropout(do_rate)(cross)]))\n",
    "        ff = Dense(ff_dim, 'relu')(x_dec); ff = Dense(d_model)(ff)\n",
    "        x_dec = LayerNormalization(epsilon=1e-6)(Add()([x_dec, Dropout(do_rate)(ff)]))\n",
    "\n",
    "    out = Dense(output_units, activation=last_activation)(x_dec)\n",
    "    return Model([enc_in, dec_in], out)\n",
    "\n",
    "enc_dim = Xe_tr.shape[2]\n",
    "dec_dim = Xd_tr.shape[2]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 10. Stage-1: Income CLASSIFIER\n",
    "# ---------------------------------------------------------------------\n",
    "clf_model = build_transformer(L, H, enc_dim, dec_dim,\n",
    "                              last_activation=\"sigmoid\", output_units=1)\n",
    "\n",
    "clf_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3, clipnorm=1.0),\n",
    "                  loss=bce,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "cb_clf = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True)] if Xe_val is not None else []\n",
    "\n",
    "clf_model.fit([Xe_tr, Xd_tr], y_flag_tr[..., None],   # add channel dim\n",
    "              validation_data=([Xe_val, Xd_val], y_flag_val[..., None])\n",
    "              if Xe_val is not None else None,\n",
    "              epochs=50, batch_size=32, shuffle=True,\n",
    "              callbacks=cb_clf, verbose=2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 11. Stage-2: Income REGRESSOR (masked pinball)\n",
    "# ---------------------------------------------------------------------\n",
    "y_inc_tr_exp  = np.repeat(y_inc_tr[..., None], Q, axis=2)\n",
    "y_inc_val_exp = np.repeat(y_inc_val[..., None], Q, axis=2) if Xe_val is not None else None\n",
    "\n",
    "reg_model = build_transformer(L, H, enc_dim, dec_dim,\n",
    "                              last_activation=\"linear\", output_units=Q)\n",
    "\n",
    "reg_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3, clipnorm=1.0),\n",
    "                  loss=masked_pinball_loss)\n",
    "\n",
    "cb_reg = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True)] if Xe_val is not None else []\n",
    "\n",
    "reg_model.fit([Xe_tr, Xd_tr], y_inc_tr_exp,\n",
    "              validation_data=([Xe_val, Xd_val], y_inc_val_exp)\n",
    "              if Xe_val is not None else None,\n",
    "              epochs=50, batch_size=32, shuffle=True,\n",
    "              callbacks=cb_reg, verbose=2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 12. Expense model\n",
    "# ---------------------------------------------------------------------\n",
    "y_exp_tr_exp  = np.repeat(y_exp_tr[..., None], Q, axis=2)\n",
    "y_exp_val_exp = np.repeat(y_exp_val[..., None], Q, axis=2) if Xe_val is not None else None\n",
    "\n",
    "exp_model = build_transformer(L, H, enc_dim, dec_dim,\n",
    "                              last_activation=\"linear\", output_units=Q)\n",
    "\n",
    "exp_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3, clipnorm=1.0),\n",
    "                  loss=pinball_loss)\n",
    "\n",
    "cb_exp = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True)] if Xe_val is not None else []\n",
    "\n",
    "exp_model.fit([Xe_tr, Xd_tr], y_exp_tr_exp,\n",
    "              validation_data=([Xe_val, Xd_val], y_exp_val_exp)\n",
    "              if Xe_val is not None else None,\n",
    "              epochs=50, batch_size=32, shuffle=True,\n",
    "              callbacks=cb_exp, verbose=2)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 13. Build encoder / decoder inputs for forecast\n",
    "# ---------------------------------------------------------------------\n",
    "def enc_block(series_scaled):\n",
    "    return np.hstack([\n",
    "        series_scaled[-L:].reshape(L,1),\n",
    "        expense_scaled[-L:].reshape(L,1),\n",
    "        dow[-L:], mon[-L:], first_of_month[-L:], holiday_flag[-L:]\n",
    "    ])[None, ...]\n",
    "\n",
    "enc_input = enc_block(income_scaled)          # ← same for all three models\n",
    "dec_dow   = np.zeros((H, 7),  dtype=np.float32)\n",
    "dec_mon   = np.zeros((H, 12), dtype=np.float32)\n",
    "\n",
    "future_dates = pd.date_range(forecast_start, forecast_end, freq=\"D\")\n",
    "dec_dow[np.arange(H), future_dates.weekday] = 1.0\n",
    "dec_mon[np.arange(H), future_dates.month-1] = 1.0\n",
    "dec_first  = (future_dates.day == 1).astype(np.float32).reshape(-1,1)\n",
    "dec_hol    = np.array([d.date() in holidays for d in future_dates], np.float32).reshape(-1,1)\n",
    "\n",
    "dec_input = np.hstack([dec_dow, dec_mon, dec_first, dec_hol])[None, ...]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 14. Forecast\n",
    "# ---------------------------------------------------------------------\n",
    "prob_salary  = clf_model.predict([enc_input, dec_input])[0].squeeze()      # (H,)\n",
    "inc_amount_s = reg_model.predict([enc_input, dec_input])[0]                # (H,Q)\n",
    "exp_pred_s   = exp_model.predict([enc_input, dec_input])[0]                # (H,Q)\n",
    "\n",
    "# Gate income amounts: prob > 0.5 ⇒ keep; else set to 0\n",
    "gate = (prob_salary > 0.5).astype(np.float32)[:, None]     # (H,1)\n",
    "inc_amount_s *= gate\n",
    "\n",
    "# Inverse scaling\n",
    "inc_amount = inc_amount_s * income_max                     # ≥ 0\n",
    "exp_amount = exp_pred_s  * (-expense_min_abs)              # ≤ 0\n",
    "\n",
    "# Ensure no positive expenses\n",
    "exp_amount = np.where(exp_amount > 0, 0, exp_amount)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 15. Results dataframe\n",
    "# ---------------------------------------------------------------------\n",
    "pred_df = pd.DataFrame({\n",
    "    \"date\":           future_dates.date,\n",
    "    \"income_prob\":    prob_salary,\n",
    "    \"income_p10\":     inc_amount[:,0],\n",
    "    \"income_median\":  inc_amount[:,1],\n",
    "    \"income_p90\":     inc_amount[:,2],\n",
    "    \"expense_p10\":    exp_amount[:,0],\n",
    "    \"expense_median\": exp_amount[:,1],\n",
    "    \"expense_p90\":    exp_amount[:,2],\n",
    "})\n",
    "\n",
    "print(pred_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Predictions saved → C:\\Users\\loics\\OneDrive\\Documents\\1. BAM\\BLOCK 5\\Assignment coding\\final_income_expense_predictions_v2.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "CSV_PATH = Path(r\"C:/Users/loics/OneDrive/Documents/1. BAM/BLOCK 5/Assignment coding/synthetic_transactions_daily_enriched.csv\")\n",
    "OUT_PATH = CSV_PATH.parent / f\"final_income_expense_predictions_v2.csv\"\n",
    "pd.DataFrame(pred_df).to_csv(OUT_PATH, index=False)\n",
    "print(f\"✔  Predictions saved → {OUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
